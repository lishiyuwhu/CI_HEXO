---
title: ALC papers NOTES
date: 2017-08-06 15:47:31
tags: ALC
---

~~æ„Ÿå†’åˆåœ¨åºŠä¸Šèººäº†å‡ å¤©, è¯¥èµ·æ¥å¹²æ´»äº†.~~


# Active Learning from Crowds with Unsure Option
Jinhong Zhong , Ke Tang  and Zhi-Hua Zhou

## Abstract

Learning from crowds, where the labels of data instances are collected using a crowdsourcing way,
å’Œä¼ ç»Ÿä¼—åŒ…åšæ³•æŠŠæ‰€æœ‰æ•°æ®éƒ½åˆ†å‘å‡ºå»ä½œæ ‡æ³¨ä¸åŒ, active learning from crowdsä¸»åŠ¨åœ°åªé€‰å–ä¸€éƒ¨åˆ†æ•°æ®åˆ†å‘å‡ºå»æ ‡æ³¨, å‡å°‘äº†æ ‡æ³¨çš„cost.æœ¬æ–‡æ›´è¿›ä¸€æ­¥çš„, å…è®¸æ ‡æ³¨è€…å›ç­”'unsure'. å‡è½»äº†æ ‡æ³¨è€…çš„è´Ÿæ‹…, æ›´èƒ½æé«˜å‡†ç¡®ç‡

<!-- more -->

## 1 Introduction

é‡‡ç”¨äº†å¤šä¸ªæ ‡æ³¨è€…ä¹‹å, ALCç®—æ³•ä¸ä»…è¦é€‰æ‹©å»æŸ¥è¯¢å“ªäº›å®ä¾‹, åŒæ—¶è¿˜è¦é€‰æ‹©ä¸ºé€‰å®šçš„å®ä¾‹å»å¯»æ‰¾æ°å½“çš„æ ‡æ³¨è€…,  is a non-trivial task. äºæ˜¯, è¡¡é‡å€™é€‰è€…æ‰€å…·æœ‰çš„ä¸“ä¸šçŸ¥è¯†(expertise), ä¹Ÿå°±æˆä¸ºäº†ç®—æ³•çš„å…³é”®. 
Active Learning from Crowds with Unsure option (ALCU),

## 2 Related Work

ç®—æ³•å…³é”®æ˜¯å¦‚ä½•å»è¡¡é‡æ ‡æ³¨è€…çš„expertise, ç°æœ‰çš„æ–¹æ³•æœ‰ä¸‰ç±»:

1. ä¸€ä¸ªæ ‡æ³¨è€…ä»¥ä¸€ä¸ªç¡®å®šçš„æ¦‚ç‡æ­£ç¡®å›ç­”, ç„¶åä»¥è¿™ä¸ªæ¦‚ç‡ä½œä¸ºæ‰€æœ‰æ ‡æ³¨è€…å¯¹æ‰€æœ‰å®ä¾‹çœŸå´å›ç­”çš„æ¦‚ç‡
2. ä¸€ä¸ªæ ‡æ³¨è€…æ­£ç¡®æ ‡æ³¨ä¸€ä¸ªå®ä¾‹ç”±ä¸¤æ–¹é¢å†³å®š, æ ‡æ³¨è€…è‡ªå·±çš„å¯é æ€§ä»¥åŠè¯¥å®ä¾‹çš„éš¾æ˜“ç¨‹åº¦.
3. ç›´æ¥ä¼°è®¡æ¯ä¸ªæ ‡æ³¨è€…å¯¹äºæ¯ä¸ªå®ä¾‹èƒ½å¤Ÿåšå‡ºæ­£ç¡®æ ‡æ³¨çš„æ¦‚ç‡, but estimating the accuracy of each annotator in each instance, which is #non-trivial# to solve.

## 3 Problem Description and Analyses

## 4 ALCU-SVM

æœ‰å‡ ç‚¹:

 - è®­ç»ƒåˆæœŸ, å…ˆå»æ‰æ­£åˆ™é¡¹æˆ–è€…è¯´æ˜¯é™åˆ¶é¡¹, å› ä¸ºæ­¤æ—¶å³ä½¿å­˜åœ¨æ ‡æ³¨è€…èƒ½å¤Ÿæ­£ç¡®æ ‡æ³¨ä¹Ÿä¸ä¸€å®šèƒ½å¤Ÿé€‰æ‹©å‡ºæ¥.
 - æ ‡æ³¨è€…å¯é æ€§æ¨¡å‹çš„æ„å»º, æ˜¯ä¸€ä¸ªç±»åˆ«ä¸å‡è¡¡é—®é¢˜(class-imbalanced). -> set weights proportional

## 5 Experiments

## 6 Conclusion and Discussion
'unsure'é€‰é¡¹èƒ½å¤Ÿæ˜¾è‘—çš„ä¼˜åŒ–ALCçš„ç»“æœ.

æœ‰å‡ ä¸ªè¿›ä¸€æ­¥çš„æ–¹å‘:

 - ALCU-SVMä»…ä»…ä¾èµ–äºæ ‡æ³¨è€…çš„åé¦ˆæ¥åˆ¤æ–­ä¸€ä¸ªæ ‡æ³¨è€…èƒ½å¦å¯ä¿¡, å½“æ ‡æ³¨è€…falsely confidentæ—¶ä¼šæœ‰ä¸å¥½çš„è¡¨ç°. æˆ‘ä»¬å¯ä»¥å†å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿåæ˜ æ ‡æ³¨è€…ä¿¡å¿ƒçš„æ¨¡å‹, å¦ä¸€æ–¹é¢, ä»¤æ¨¡å‹æ›´åŠ é²æ£’, èƒ½å¤Ÿå¯¹é”™è¯¯çš„æ ‡æ³¨æœ‰ä¸€å®šå®¹å¿ç¨‹åº¦.
 - softmax functionçš„ä½¿ç”¨
 - æœ€ä¼˜æ ‡æ³¨è€…çš„é€‰æ‹©
 - å½“æœ‰å¤§é‡æ ‡æ³¨è€…æ—¶, ç®—æ³•çš„scalability
 
 
# References

# Finite-time Analysis of the Multiarmed Bandit Problem


## Abstract
å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªå›°å¢ƒ, éœ€è¦åœ¨exploringå’ŒæŒ‰ç»éªŒçš„æœ€ä½³è¡ŒåŠ¨ä¹‹é—´æ‰¾å¯»ä¸€ä¸ªå¹³è¡¡.
ä¸€ä¸ªç®€å•çš„ä¾‹å­å°±æ˜¯å¤šè‡‚èµŒåšæœº, 'regret'å¯ä»¥æœ‰å¾ˆå¥½çš„æ•ˆæœ

æ‰€è°“å¤šè‡‚èµŒåšæœº, 

>ç®€çŸ­æ¥è¯´ï¼Œåœ¨åˆé€‚çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¿è¯ï¼Œå­˜åœ¨è¿™æ ·ä¸€ç§ç®—æ³•ï¼Œä½¿å¾—æˆ‘ä»¬çš„â€œåæ‚”ç¨‹åº¦â€æ˜¯æœ€ä½çš„ã€‚è­¬å¦‚ï¼Œå½“ä¸€åèµŒå¾’é¢å¯¹è¿‘ä¼¼ä¸ºï¼ˆé™æ€ï¼‰éšæœºè¿‡ç¨‹çš„è€è™æœºæ—¶ï¼Œä»–éœ€è¦ä¸€ç§ç­–ç•¥ï¼Œæœ€å¤§åŒ–ä»–è‡ªå·±çš„åˆ©ç›Šã€‚å†å½¢è±¡ä¸€ç‚¹ï¼šèµŒå¾’è¿›äº†èµŒåœºï¼Œé¢å¯¹ä¸€åƒå°æœºå™¨ï¼Œæ¯å°æœºå™¨çš„ä¸­å¥–ç‡æ˜¯ä¸€ä¸ªå®šå€¼ã€‚å½“ç„¶ï¼ŒèµŒå¾’ä¸çŸ¥é“ä¸­å¥–ç‡çš„æ•°å€¼ï¼Œä»–åªèƒ½é ä¸æ–­èµŒåšæ¥æµ‹è¯•æ¯å°æœºå™¨çš„èµ¢é¢ã€‚æµ‹å¾—è¶Šå¤šï¼Œå¯¹ä¸­å¥–ç‡çš„ä¼°è®¡å°±è¶Šå‡†ã€‚é‚£ä¹ˆï¼ŒèµŒå¾’ç©¶ç«Ÿæ˜¯åº”è¯¥ä½•æ—¶åˆ°åé¢æ²¡è¯•è¿‡çš„æœºå™¨ç¢°ç¢°è¿æ°”ï¼Ÿä½•æ—¶åº”è¯¥ç»§ç»­åœ¨35å·æœºä¸Šç»§ç»­ä¸‹æ³¨ï¼ˆå› ä¸ºåˆšæ‰åœ¨35å·ä¸‹æ³¨ä¸‰æ¬¡éƒ½èµ¢äº†ï¼‰ï¼ŸæŠ‘æˆ–æ¢å›ä¸€å¼€å§‹çš„2å·æœºï¼ˆä¹Ÿè®¸2å·æœºçš„èµ¢é¢æœ€å¤§ï¼Œåªæ˜¯èµŒå¾’ä¸€æ—¶è¿æ°”ä¸å¥½è¾“æ‰äº†å‡ è½®ï¼‰ï¼Ÿ
ä½œè€…ï¼šFilestorm
é“¾æ¥ï¼šhttps://www.zhihu.com/question/19784847/answer/12953467

æ–‡ä¸­æå‡ºçš„ç­–ç•¥, ä¸åªæ˜¯æ¸è¿›çš„, è€Œæ˜¯ä¸€è‡´çš„è¾¾åˆ°logarithmic regret





#  Multiple-Instance Active Learning

[Settles et al., 2008] B. Settles, M. Craven, and S. Ray.
Multiple-instance active learning. In NIPS, pages 1289â€“
1296, 2008.

---

```

multiple-instance (MI)é—®é¢˜, å®ä¾‹ä»¬è¢«åˆ†å‰²æˆbags, è€Œä¸å†æ˜¯ç‹¬ç«‹çš„å®ä¾‹. å¯¹äºMLé—®é¢˜æ¥è¯´,æ¯ä¸ªåŒ…éƒ½æœ‰ä¸€ä¸ªè®­ç»ƒæ ‡è®°ï¼Œè€ŒåŒ…ä¸­çš„ç¤ºä¾‹æ˜¯æ²¡æœ‰æ ‡è®°çš„ï¼›å¦‚æœåŒ…ä¸­è‡³å°‘å­˜åœ¨ä¸€ä¸ªæ­£æ ‡è®°çš„ç¤ºä¾‹ï¼Œåˆ™åŒ…è¢«èµ‹äºˆæ­£æ ‡è®°ï¼›è€Œå¯¹äºä¸€ä¸ªæœ‰è´Ÿæ ‡è®°çš„åŒ…ï¼Œå…¶ä¸­æ‰€æœ‰çš„ç¤ºä¾‹å‡ä¸ºè´Ÿæ ‡è®°ã€‚ï¼ˆè¿™é‡Œè¯´åŒ…ä¸­çš„ç¤ºä¾‹æ²¡æœ‰æ ‡è®°ï¼Œè€Œåé¢åˆè¯´åŒ…ä¸­è‡³å°‘å­˜åœ¨ä¸€ä¸ªæ­£æ ‡è®°çš„ç¤ºä¾‹æ—¶åŒ…ä¸ºæ­£æ ‡è®°åŒ…ï¼Œæ˜¯ç›¸å¯¹è®­ç»ƒè€Œè¨€çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è®­ç»ƒçš„æ—¶å€™æ˜¯æ²¡æœ‰ç»™ç¤ºä¾‹æ ‡è®°çš„ï¼Œåªæ˜¯ç»™äº†åŒ…çš„æ ‡è®°ï¼Œä½†æ˜¯ç¤ºä¾‹çš„æ ‡è®°æ˜¯ç¡®å®å­˜åœ¨çš„ï¼Œå­˜åœ¨æ­£è´Ÿç¤ºä¾‹æ¥åˆ¤æ–­æ­£è´Ÿç±»åˆ«ï¼‰

å…³äºå¤šç¤ºä¾‹é—®é¢˜æ€ä¹ˆæ±‚è§£ï¼Œå‡å¦‚è¯´æ‰€æœ‰çš„æ ·æœ¬æ ‡è®°éƒ½å·²ç»çŸ¥é“äº†ï¼Œé‚£å°±æ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ çš„é—®é¢˜äº†ï¼Œç”¨SVMï¼Œadaboostä¹‹ç±»çš„éƒ½å¯ä»¥åšã€‚ç°åœ¨çš„å›°éš¾æ˜¯ï¼Œæœ‰å¾ˆå¤šæ ·æœ¬çš„æ ‡è®°æˆ‘ä»¬ä¸çŸ¥é“ã€‚å¯¹äºè´Ÿæ ·æœ¬åŒ…æ¥è¯´å°±æ— æ‰€è°“äº†ï¼Œé‡Œé¢æ¯ä¸ªæ ·æœ¬é‚£éƒ½æ˜¯è´Ÿæ ‡è®°ï¼Œè¿™ä¸ªæ˜¯æ˜ç¡®çš„ã€‚é—®é¢˜å‡ºåœ¨æ­£æ ·æœ¬åŒ…ä¸Šé¢ï¼Œæ¯ä¸ªæ­£æ ·æœ¬åŒ…é‡Œåªèƒ½ä¿è¯æœ‰ä¸€ä¸ªæ˜¯æ­£æ ·æœ¬ï¼Œå…¶ä»–çš„æ˜¯æ­£æ˜¯è´Ÿå°±ä¸çŸ¥é“äº†ï¼Œå…³é”®æ˜¯åˆ°åº•æ˜¯å“ªä¸ªæ ·æœ¬æ˜¯æ­£çš„å‘¢ï¼Ÿè¿™ä¸ªä¹Ÿæ˜¯ä¸æ¸…æ¥šçš„ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•å…¶å®æŒºç›´æ¥çš„ï¼šè¿­ä»£ä¼˜åŒ–ï¼ˆalternative optimization)ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å…ˆå‡è®¾å·²ç»çŸ¥é“äº†æ‰€æœ‰æ ·æœ¬çš„æ ‡è®°ï¼Œé‚£ä¹ˆå°±å¯ä»¥é€šè¿‡æŸç§ç›‘ç£å­¦ä¹ çš„æ–¹æ³•å¾—åˆ°ä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œé€šè¿‡è¿™ä¸ªæ¨¡å‹æˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œç„¶åæ›´æ–°å®ƒä»¬çš„æ ‡è®°ï¼Œæˆ‘ä»¬åˆå¯ä»¥æ‹¿è¿™ä¸€æ¬¡æ–°å¾—åˆ°çš„æ ‡è®°é‡æ–°è®­ç»ƒåˆ†ç±»æ¨¡å‹äº†ã€‚æ‰€ä»¥æ•´ä¸ªä¼˜åŒ–è¿‡ç¨‹åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šç›‘ç£å­¦ä¹ ï¼Œæ ‡è®°æ›´æ–°ã€‚

è¿™é‡Œè¿˜æœ‰ä¸€äº›åœ°æ–¹éœ€è¦æ³¨æ„ï¼š

ç¬¬ä¸€ç‚¹ï¼Œ è®­ç»ƒç›‘ç£å­¦ä¹ çš„æ¨¡å‹çš„æ—¶å€™ï¼Œåªä»æ­£æ ·æœ¬åŒ…é‡ŒæŒ‘é€‰è¢«é¢„æµ‹çš„â€œæœ€åƒæ­£ç¡®â€(ä¹Ÿå°±æ˜¯åˆ†ç±»å¾—åˆ†æœ€é«˜)çš„é‚£ä¸€ä¸ªï¼Œæ­£æ ·æœ¬åŒ…é‡Œé¢å…¶ä»–çš„æ ·æœ¬ï¼Œä¸ç®¡é¢„æµ‹å‡ºæ¥æ˜¯æ­£çš„è¿˜æ˜¯è´Ÿçš„éƒ½ä¸è¦äº†ã€‚è¿™æ˜¯å› ä¸ºï¼Œå…¶å®å¤šç¤ºä¾‹çš„é—®é¢˜ä¹Ÿå¯ä»¥æè¿°ä¸ºï¼Œæ­£æ ·æœ¬åŒ…é‡Œé¢â€œæœ€æ­£ç¡®â€çš„ä¸€ä¸ªæ ·æœ¬æ ‡è®°æ˜¯æ­£çš„ï¼Œè·Ÿå…¶ä»–æ ·æœ¬æ— å…³ã€‚æ‰€ä»¥ï¼Œè¿™ç§é€‰æ‹©ç­–ç•¥æ°æ°æ˜¯ç¬¦åˆé—®é¢˜å®šä¹‰çš„ã€‚

ç¬¬äºŒç‚¹ï¼Œå¦‚æœè´Ÿæ ·æœ¬è¶³å¤Ÿå¤šçš„è¯ï¼Œå¯ä»¥åªæŒ‘é€‰æ¯ä¸ªè´Ÿæ ·æœ¬åŒ…é‡Œé¢è¢«é¢„æµ‹â€œæœ€åƒæ­£ç¡®"çš„ä¸€ä¸ªæ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™æ ·å­çš„è´Ÿæ ·æœ¬ä¹Ÿå«åšhard sampleæˆ–è€…most violated sampleã€‚å®è·µä¸Šæ¥è¯´ï¼Œå®ƒä»¬å¯¹äºæ¨¡å‹å¿«é€Ÿæ”¶æ•›æ˜¯æœ€æœ‰æ•ˆçš„ã€‚

```

---

#  Active Learning Using Pre-clustering

[Nguyen and Smeulders, 2004] H. T. Nguyen and A. Smeul-
ders. Active learning using pre-clustering. In ICML,
page 79. ACM, 2004.

---

é€šå¸¸çš„active learingæ”¶é›†æ•°æ®çš„æ–¹æ³•æ˜¯é€‰æ‹©é‚£äº›é è¿‘åˆ†ç±»è¾¹ç•Œçš„å®ä¾‹, æ–‡ä¸­æå‡ºäº†å°†èšç±»é€šactive learingç›¸ç»“åˆçš„æ–¹æ³•.

è¿™ä¸ªç®—æ³•é¦–å…ˆåœ¨èšç±»ä»£è¡¨ä¸Šæ„é€ äº†ä¸€ä¸ªåˆ†ç±»å™¨, ç„¶åå°†åˆ†ç±»ç»“æœé€šè¿‡å™ªéŸ³æ¨¡å‹ä¼ æ’­åˆ°å…¶ä»–æ ·æœ¬. è¿™ä¸ªç®—æ³•èƒ½å¤Ÿé€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§å…¸å‹çš„çš„ä¾‹å­, è€Œä¸”èƒ½é¿å…åœ¨ç›¸åŒçš„èšç±»ä¸­é‡å¤çš„æ‰“æ ‡ç­¾. åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­, èšç±»ç”±ç²—åˆ°ç»†, ä»¥æœŸæœ›èƒ½å¹³è¡¡å¥½èšç±»è§„æ¨¡å’Œæ ‡ç­¾çš„å‡†ç¡®æ€§.

è¿‘å¹´æ¥, åŠç›‘ç£å­¦ä¹ , ä»¥åŠåœ¨å°‘é‡æ‰“æ ‡åˆå§‹æ•°æ®èº«ä¸Šæ”¹å–„ç›‘ç£å­¦ä¹ å¾ˆç«. è®¸å¤šæ–¹æ³•æ”¹è¿›ç›‘ç£å­¦ä¹ é€šè¿‡ä½¿ç”¨æœªæ‰“æ ‡æ•°æ®, å¦ä¸€äº›åœ¨æ‰¾å¯»å¥½çš„ç­–ç•¥æ¥é€‰æ‹©åˆé€‚çš„æ•°æ®æ¥æ‰“æ ‡.
,ä¹Ÿå°±æ˜¯active learing.

ç°ä»Šactive learningæŠ€æœ¯å¯ä»¥æŒ‰ç…§åº•å±‚çš„å­¦ä¹ æ–¹æ³•åˆ†ç±», åƒnaive Bayes, SVM. 
naive Bayesåˆ†ç±»å™¨æœ‰ä¸¤ä¸ªé—®é¢˜,.é¦–å…ˆ, åˆ†ç±»å™¨å‡è®¾äº†featureçš„ç‹¬ç«‹, é€šå¸¸æ¥è®²è¿™å¤ªå¼ºäº†. å…¶æ¬¡, naive Bayesæ˜¯ä¸€ä¸ªåŸºäºlikelihoodä¼°è®¡çš„ç”Ÿæˆæ¨¡å‹, åœ¨active learingä¸­, ç”±äºdataä¸æ˜¯éšæœºé€‰å–çš„, è¿™ä¸ªä¼°è®¡é€šå¸¸æ˜¯ä¸å‡†ç¡®çš„.

è¦ç‚¹æ˜¯å»é€‰æ‹©æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®. è®¸å¤šç®—æ³•é€‰å–çš„æ˜¯æœ€é è¿‘åˆ†ç±»è¾¹ç•Œä¹Ÿå°±æ˜¯æœ€éš¾ä»¥ç¡®å®šçš„sample, æˆ‘ä»¬æŠŠå®ƒå«åšclosest-to-boundaryå‡†åˆ™. ä¸€äº›å…¶ä»–çš„å‡†åˆ™æ¥è‡ªäºSVM, é€‰æ‹©é‚£äº›èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦å‡å°marginçš„ä¾‹å­.

closest-to-boundaryæ–¹æ³•å¿½ç•¥äº†å¯¹active learingå¾ˆæœ‰å¸®åŠ©çš„å…ˆéªŒ data distribution. 
æœ¬æ–‡ä¸­, èšç±»çš„ä¿¡æ¯åœ¨ä¸¤ä¸ªæ–¹é¢ä¸Šå¸®åŠ©äº†active Learning: é¦–å…ˆ, ä½äºèšç±»ä¸­å¿ƒçš„ä»£è¡¨sampleæ›´åŠ é‡è¦, åº”è¯¥ä¼˜å…ˆé€‰æ‹©å»æ‰“æ ‡; å…¶æ¬¡, ä½äºåŒä¸€èšç±»çš„sampleå¯èƒ½åº”è¯¥å…·æœ‰ç›¸åŒçš„æ ‡ç­¾, è¿™æ ·å¯ä»¥æ˜¾è‘—å‡å°‘æ‰“æ ‡æ•°é‡.


#  Active Learning by Querying Informative and Representative Examples

[Huang et al., 2010] S.-J. Huang, R. Jin, and Z.-H. Zhou.
Active learning by querying informative and representa-
tive examples. In NIPS, pages 892â€“900, 2010.

---

è¿™ç¯‡æ˜¯å‘¨å¿—åè€å¸ˆçš„, æƒ³æƒ³æˆ‘ä¹Ÿæ˜¯çœ‹ç€è¥¿ç“œä¹¦æ‰ç®—æ­£å¼å…¥é—¨çš„.

å¤§å¤šæ•°active learningæ–¹æ³•é€‰æ‹©informativeæˆ–è€…representativeçš„æœªæ‰“æ ‡å®ä¾‹å»æŸ¥è¯¢å®ƒä»¬çš„æ ‡ç­¾. è™½ç„¶å·²ç»æœ‰ç®—æ³•å°†è¿™ä¸¤è€…ç»“åˆèµ·æ¥æŸ¥è¯¢, ä½†ä»–ä»¬é€šå¸¸æ˜¯ad hoc(ç‚¹å¯¹ç‚¹,ç‰¹åˆ«çš„)çš„å»æ‰¾åˆinformativeåˆrepresentative.

æœ¬æ–‡, ä»ä¸€ä¸ªç»™å®šçš„æ± ä¸­é€‰æ‹©ä¸€ä¸ªæœªæ‰“æ ‡å®ä¾‹å»æ‰‹åŠ¨æ‰“æ ‡, pool-basedåŸºäºæ± åŒ–çš„active learning. ä¸¤ä¸ªä¸»è¦çš„å‡†åˆ™, informativeness & representativeness. informativenessè¡¡é‡äº†ä¸€ä¸ªå®ä¾‹å‡å°‘ç»Ÿè®¡æ¨¡å‹ä¸ç¡®å®šæ€§çš„èƒ½åŠ›, è€Œrepresentativenessè¡¡é‡äº†ä¸€ä¸ªå®ä¾‹æ˜¯å¦èƒ½å¤Ÿå¾ˆå¥½çš„ä»£è¡¨æœªæ‰“æ ‡æ•°æ®æ•´ä½“patternçš„èƒ½åŠ›. å¤§å¤šæ•°active learningç®—æ³•éƒ½åªå…³æ³¨äº†å…¶ä¸­çš„ä¸€ç‚¹: å…³æ³¨informativenessé€šå¸¸ä¸èƒ½åˆ©ç”¨æœªæ‰“æ ‡æ•°æ®çš„ç»“æ„ä¿¡æ¯, äº§ç”Ÿbias, å¾—åˆ°ä¸å¥½çš„performance; å…³æ³¨representativeness, éœ€è¦åœ¨å¾—å‡ºæœ€ä¼˜åˆ†ç±»è¾¹ç•Œä¹‹å‰å°±æŸ¥è¯¢å¤§å¤§å¤§å¤§å¤§é‡çš„å®ä¾‹. 

QUerying Informative and Representative Examples -> QUIRE method
åŸºäºmin-max view of active learning. è¡¡é‡äº†ä¸€ä¸ªå®ä¾‹çš„ informativeness and representativeness, informativenessç”±åŸºäºå·²æ‰“æ ‡æ•°æ®çš„ä¼°è®¡,  representativenessç”±åŸºäºæœªæ‰“æ ‡çš„æ•°æ®ä¼°è®¡.

æ‰€è°“informativeness & representativeness

![](http://otivusbsc.bkt.clouddn.com/4e3b9e6f-b5bb-45bd-874e-507320cf0157)

å…·ä½“æ¥è¯´

![](http://otivusbsc.bkt.clouddn.com/c15e4831-2afa-40d1-bc19-5fd6289df08e)


---

#  Toward optimal active learning through monte carlo estimation of error reduction

>Existing active learning approaches can be categorized as using a single annotator [Roy and McCallum, 2001] or using multiple annotators [Dekel et al., 2012].

[Roy and McCallum, 2001] N. Roy and A. McCallum. To-
ward optimal active learning through monte carlo estima-
tion of error reduction. ICML, 2001.

---

ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ä¸­, æ— è®ºæ˜¯ä»€ä¹ˆæ ·çš„training dataéƒ½å»ç”¨æ¥train parameters. ä¸ä¹‹ç›¸å, active learningè‡ªç”±çš„å»é€‰æ‹©å“ªäº›æ•°æ®èƒ½åŠ å…¥è¿›training setä¸­. ä¸€ä¸ªactive learningç®—æ³•é€šå¸¸ä»å¾ˆå°‘çš„æ‰“å·²æ ‡æ•°æ®å¼€å§‹, å°å¿ƒçš„é€‰æ‹©éœ€è¦æ‰“æ ‡çš„æ•°æ®, ä»è¿™äº›ä¹‹ä¸­å¾—åˆ°ç»“æœ, åœ¨ä½¿ç”¨æ–°å¾—åˆ°çš„çŸ¥è¯†å»å¯»æ±‚ä¸‹ä¸€æ­¥æ‰“æ ‡çš„æ•°æ®. è¿™æ ·, å°±èƒ½æœŸæœ›ä»¥ä¸€ä¸ªå°çš„labled setå¾—åˆ°å¥½çš„performance. å°¤å…¶æ˜¯, å½“æœªæ‰“æ ‡æ•°æ®cheap & plentiful, æ‰“æ ‡è¿™ä»¶äº‹åˆè‰°éš¾è€Œä¸”èŠ±æ—¶é—´.

è¿™ç¯‡paperæå‡ºçš„æ–¹æ³•æ˜¯, ä»¥æœ€å°åŒ–æœªæ¥æµ‹è¯•é›†ä¸Šçš„errorä½œä¸ºæœ€ä¼˜å‡†åˆ™, ä½†æ˜¯, ä½¿ç”¨è’™å¡ä¼°è®¡å»è§£å†³practicality problem. future error rateçš„ä¼°è®¡,  either by log-loss,using the entropy of the posterior class distribution on a sample of the unlabelled examples, or by 0-1 loss, using the posterior probabilities of the most probable class for the sampled unlabelled examples.

æ¯è½®ä¹‹å, é€‰å–ä¸€ä¸ªsampleå»æ‰“æ ‡, ç„¶åå†å»ä¼°è®¡future error rate

ä»…ä»…æ·»åŠ ä¸€ä¸ªsampleçš„è®­ç»ƒ, å…¶å®å¯ä»¥æ˜¯å¾ˆæœ‰æ•ˆçš„, æƒ³ä¸€æƒ³SVMå°±èƒ½æ˜ç™½

---

# 5. Selective sampling and active learning from single and multiple teachers

>Existing active learning approaches can be categorized as using a single annotator [Roy and McCallum, 2001] or using multiple annotators [Dekel et al., 2012].

[Dekel et al., 2012] O. Dekel, C. Gentile, and K. Sridharan.
Selective sampling and active learning from single and
multiple teachers. JMLR, 13(1):2655â€“2697, 2012.

---
è¿™ç¯‡æœ‰40+é¡µ ğŸ¤¦ğŸ¤¦ğŸ¤¦ğŸ¤¦ğŸ¤¦ğŸ¤¦ğŸ¤¦ğŸ¤¦æš‚æ—¶å…ˆçœ‹ä¸ªå¤§æ¦‚å§

ä¸€ä¸ªæ–°çš„online learning algorithm(å«åœ¨çº¿å­¦ä¹ æ€ä¹ˆæ„Ÿè§‰è¿™ä¹ˆåˆ«æ‰­)é€‰æ‹©samplingçš„æ¡†æ¶, å…¶ä¸­labelåœ¨æ­ç¤ºä¹‹å‰å¿…é¡»è¢«ä¸»åŠ¨æŸ¥è¯¢è¿‡(where labels must be actively queried before they are revealed).å¯¹regret,ä»¥åŠæŸ¥è¯¢lableçš„æ•°é‡(when faced with an adaptive adversarial strategy of generating the instances)åšäº†é™åˆ¶. å¤šæ•™å¸ˆæƒ…å½¢ä¸‹, 
ç”±äººæ‰‹å·¥ç”Ÿæˆlabelæ˜¯å¾ˆexpensiveçš„. active learningçš„èŒƒå¼æ˜¯å»ºç«‹è¿™æ ·ä¸€ä¸ªæ¦‚å¿µ: æˆ‘ä»¬åº”è¯¥åªå»å–å¾—é‚£äº›èƒ½å¤Ÿåˆ‡å®çš„æ”¹è¿›predictionçš„label. 

online selective sampling:  a repeated game between a learner and an adversary. ~çœ‹äº†è¿™ä¸ªonline selective sampling settingçš„æ¦‚å¿µ, æœ‰ç‚¹GANçš„æ„Ÿè§‰~
åœ¨ç¬¬$t$è½®, adversaryç»™learnerä¸€ä¸ªå®ä¾‹$x\_t$, ç„¶ålearnerå›åº”ä¸€ä¸ªé¢„æµ‹çš„äºŒåˆ†ç±»label$-1 ,+1$. learnerå’Œä¸€ä¸ªçŸ¥é“æ¯ä¸ªå®ä¾‹æ­£ç¡®labelçš„teacherç›¸è¿æ¥. learnerå¿…é¡»è¦å»å†³å®šæ˜¯å¦å»èŠ±è´¹å•ä½costå»å‘teacheræŸ¥è¯¢æ­£ç¡®çš„label. å¦‚æœlearnerå†³å®šå»æŸ¥è¯¢,  å°±èƒ½å¾—åˆ°æ­£ç¡®labelç„¶åæ”¹è¿›ä»¥åçš„é¢„æµ‹. å½“æˆ‘ä»¬è¦å»è¡¡é‡é¢„æµ‹çš„accuracyæ—¶, æˆ‘ä»¬è¦è€ƒå¯Ÿæ‰€æœ‰çš„label, æ— è®ºæ˜¯å¦è¢«æŸ¥è¯¢è¿‡. learneræœ‰ä¸¤ä¸ªç›®æ ‡: ç²¾å‡†é¢„æµ‹å’Œå°‘é‡æŸ¥è¯¢

### The Single Teacher Case

æˆ‘ä»¬çš„å‡è®¾åªæœ‰$||x|| <= 1$, è€Œä¹‹å‰çš„å·¥ä½œéƒ½éœ€è¦æœ‰ä¸€ä¸ªå¾ˆå¼ºçš„å‡è®¾.

The single teacher algorithm is a margin-based selective sampling procedure

### The Multiple Teacher Case

Kä¸ªteacher, æ¯ä¸ªéƒ½æœ‰è‡ªå·±ä¸åŒçš„expertise, è€Œç®—æ³•å¹¶ä¸çŸ¥é“teacherå…·ä½“çš„æƒ…å†µ, åªèƒ½å¤Ÿä»teacheræä¾›çš„labelä¸­æ¨æ–­. ç»è¿‡ä¸€ç³»åˆ—åˆ¤æ–­, åœ¨é¢å¯¹æ–°çš„æŸ¥è¯¢æ—¶, ç®—æ³•è¦å†³å®šæŠŠè¿™ä¸ªæŸ¥è¯¢å‘é€ç»™å“ªäº›teacheræ›´ä¸ºåˆé€‚. æ³¨æ„è¿™é‡Œ, teacheræä¾›çš„ä»…ä»…æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»label, è€Œä¸æ˜¯æ¨æ–­reliability rate. è™½ç„¶è¿™æ ·çœ‹èµ·æ¥æ›´åŠ å¯é .
~~é‚£ä¸ºå•¥ä¸å»è¿™ä¹ˆåšå•Š~~
æ¯ä¸ªteacheréƒ½æœ‰ä¸€ä¸ªéšè—çš„confidence

ç°åœ¨, å°±ä¸å¤ªå®¹æ˜“å»è¡¡é‡learnerçš„performanceäº†, å› ä¸ºKä¸ªteacherç»™å‡ºçš„ç»“æœæœ‰æ­£æœ‰è´Ÿ, å°±è®¾å®šä¸€ä¸ªconfidenceçš„é™ç•Œ, åªé€‰å–é‚£äº›æœ‰expertiseè€Œè¶³å¤Ÿè‡ªä¿¡çš„teacher.

---

# Ensemble methods: foundations and algorithms

> The former is regarded as traditional active learning and the latter, which is essentially a kind of ensemble method [Zhou, 2012].(æŒ‡çš„æ˜¯å¤šæ•™å¸ˆ))

[Zhou, 2012] Z.-H. Zhou. Ensemble methods: foundations
and algorithms. CRC Press, 2012.

é›†æˆæ–¹æ³•è®º, ä¸€æœ¬å¤§éƒ¨å¤´, å…¶ä¸­boostingå’Œbaggingä¸¤ç« æ‰¾æ—¶é—´çœ‹çœ‹å§

---

# Active Visual Recognition with Expertise Estimation in Crowdsourcing

> Hence, estimating the expertise of candidate annotators plays a key role
in ALC algorithms. So far, much effort has been made toward
this purpose [Long et al., 2013] [Yan et al., 2011].

[Long et al., 2013] C. Long, G. Hua, and A. Kapoor. Ac-
tive visual recognition with expertise estimation in crowd-
sourcing. In ICCV, pages 3000â€“3007. IEEE, 2013.

---

è™½ç„¶é€šè¿‡ä¼—åŒ…å¯ä»¥å¾ˆcheapçš„å¤§é‡å¾—åˆ°labeledçš„æ•°æ®, ä½†æ”¶é›†åˆ°çš„lable couldbe very noisy. æ‰€ä»¥éœ€è¦å¯¹æ‰“æ ‡è€…expertiseè¿›è¡Œè¡¡é‡. é«˜æ°´å¹³çš„æ‰“æ ‡è€…æ›´ä¸å®¹æ˜“äº§ç”Ÿlabel noises

ä¹‹å‰çš„å…³äºæ‰“æ ‡è€…expertiseå»ºæ¨¡ä¸»è¦åˆ†ä¸¤ç§:
  - é€šè¿‡é‡‡ç”¨ä¸€ä¸ªé¢„å…ˆæ‰“å¥½æ ‡çš„gold standard dataset ~~æ€•ä¸æ˜¯é‡‘æ ‡ç”ŸæŠ½å“Ÿ~~ æ¥è¯„ä¼°labelers. å½“ä¸€ä¸ªlabelerèƒ½æŒç»­å›ç­”å‡ºä¸é‡‘æ ‡æ•°æ®é›†ç›¸æ‚–çš„ç­”æ¡ˆæ—¶, å°±å¯ä»¥è®¤ä¸ºè¿™ä¸ªlabelerçš„ç­”æ¡ˆä¸é è°±
  -  ä»å¤šä¸ªlabelerä¸­è·å¾—å…¨éƒ¨data sampleçš„å›ç­”, ç„¶åå®æ—¶æˆ–è€…åæœŸvote. è¿™éœ€è¦å¤§å¤šæ•°çš„labeleréƒ½å¾ˆé è°±

ç¬¬ä¸€ä¸ªæ–¹æ³•å¯ä»¥å¦‚æˆ‘ä»¬æ‰€æ„¿çš„è¯•è¯•è¯„ä¼°labelers, ä½†æ˜¯éœ€è¦é¢„æ‰“æ ‡çš„æ•°æ®é›†. ç¬¬äºŒå¾ˆæ¸…æ™°çš„è¯„ä¼°ç§æ–¹æ³•, åˆ™æ˜¯å…³æ³¨labelçš„noise. ä»–å¹¶æ²¡æœ‰å¾ˆæ˜æ™°åœ°å»è¯„ä¼°labeler.ä½†ä»–ä»¬éƒ½å¾ˆad hoc ~~å¤§æ¦‚è¿™é‡Œå¯ä»¥è®¤ä¸ºæ˜¯ ä¸“æœ‰çš„ ?~~, ç¼ºä¹ä¸€ä¸ªå°†labelersçš„å…¨å±€è¯¯å·®å’Œexpertiseæ°´å¹³ç»“åˆèµ·æ¥çš„æ–¹æ³•.

æœ¬æ–‡åœ¨æ²¡æœ‰é‡‘æ ‡æ•°æ®é›†çš„æƒ…å†µä¸‹, æå‡ºäº†ä¸€ä¸ªBayesianæ¨¡å‹, ç”¨æ¥å­¦ä¹ åŸºäºmultiple noisy labelsçš„é«˜æ–¯è¿‡ç¨‹åˆ†ç±»å™¨. å…¶æ¬¡, æ„å»ºäº†an active classifier learning system which determines which users to label which unlabeled examples, ä¸ä»…å¯ä»¥é€‰æ‹©sample, æ›´å¯ä»¥ä¸ºå…¶é€‰æ‹©åˆé€‚çš„labeler

---

# 8. Active Learning from Crowds

> The former is regarded as traditional active learning and the latter, which is essentially a kind of ensemble method [Zhou, 2012].(æŒ‡çš„æ˜¯å¤šæ•™å¸ˆ))

[Yan et al., 2011] Y. Yan, G. M. Fung, R. Rosales, and J. G.
Dy. Active learning from crowds. In ICML, pages 1161â€“
1168, 2011.

---

1. å»æ”¶é›†ä¸€ä¸ª golden ground-truth, åœ¨ä¸€äº›é—®é¢˜ä¸­æ˜¯å¾ˆéš¾ç”šè‡³ä¸å¯èƒ½çš„.
2. ä¸€ä¸ªannotatorä¸ä¸€å®šèƒ½å¤Ÿæœ‰èƒ½åŠ›å‡†ç¡®æ ‡æ³¨æ‰€æœ‰çš„data
3. æœ‰äº›æ—¶å€™, ä»è®¸å¤šä¸æ˜¯é‚£ä¹ˆä¸“ä¸šçš„annotatorä¸­å¾—å‡ºç»“æœ, ä¼šæ¯”ä»ä¸€ä¸ªä¸“å®¶å¾—åˆ°ç»“æœæ›´cheap
4. åˆä½œå’ŒçŸ¥è¯†å…±äº«, ä½¿å¾—ç»“åˆå¤šannotatorçš„æŠ€æœ¯ä¼šæ›´å¿…è¦ ~~è¿™è¯å¥½ç©º~~


å¦‚æœæˆ‘ä»¬æƒ³è¦æœ‰æ•ˆçš„æ ‡è®°(learn the most at a given cost), ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ åªéœ€è¦æ‰¾å‡ºé‚£ä¸ªæœ€æœ‰ç”¨çš„data point. ä½†åœ¨å¦‚ä»Šçš„å¤šlabeleræƒ…æ™¯ä¸‹, å°±é¢ä¸´ç€ä¸€ä¸ªé—®é¢˜: å¤šæ•™å¸ˆæƒ…å†µä¸‹, è¿™ä¸ª'æœ€'è¯¥æ€ä¹ˆç¡®ç«‹

æœ¬æ–‡å…³é”®å°±æ˜¯æˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿä¸ºä¸€ä¸ªç‰¹å®šçš„taské€‰æ‹©åˆé€‚çš„labeler

ä¼˜åŒ–åˆ†ä¸¤æ­¥, å…ˆé€‰ç‚¹, å†ä»¥æ­¤ç‚¹å»é€‰labeler
1. Which new training point to pick?
    æœ€ç®€å•çš„ç­–ç•¥å°±æ˜¯ uncertainty sampling, é€‰é‚£äº›æˆ‘ä»¬æœ€ä¸ç¡®å®šçš„. åœ¨è¿™ä¸ªç®€å•çš„äºŒåˆ†ç±»ç­–ç•¥ä¸­, é€‰æ¦‚ç‡æ¥è¿‘1/2çš„é‚£äº›. 
2. Which expert to pick?
    æŒ‰confidenceå»æ‰¾.  optimizationæ–¹é¢, å°†åŸé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªbi-convexé—®é¢˜, å°±èƒ½ä½¿ç”¨ç‰›é¡¿æ³•å»åšä¼˜åŒ–äº†


# Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers


>First, in their pioneering work, Sheng and Provost [Sheng
etal., 2008]assume that an annotator gives correct labels with
a certain probability, and the probability is assumed to be the
same for all annotators in all instances.

[Sheng et al., 2008] V. S. Sheng, F. Provost, and P. G. Ipeiro-
tis. Get another label? improving data quality and da-
ta mining using multiple, noisy labelers. In KDD, pages
614â€“622. ACM, 2008.

---

**è¿™ç¯‡è®²çš„æ˜¯æˆ‘ä¹‹å‰å¾ˆåœ¨æ„çš„ä¸€ä¸ªç‚¹, æ€ä¹ˆä»å¤§é‡labeledä½†æ˜¯labelerså¯èƒ½å¹¶ä¸å…·æœ‰å¾ˆå¼ºçš„expertiseçš„dataä¸­å¾—åˆ°ç»“æœ. å°±æ¦‚å¿µä¸Šæ¥è¨€å¾ˆæœ‰ç§ensembleçš„æ„Ÿè§‰.**

**çœ‹åˆ°è¿™é‡Œæœ‰ä¸ªidea, ä»ALCUæƒ³å¼€, è¦æ˜¯ä¸€ä¸ªæ²¡æœ‰å¾ˆå¤šexpertiseçš„labeler, åœ¨å®ƒä¸ç†Ÿæ‚‰çš„é¢†åŸŸåšå‡ºäº†ç¡®åˆ‡å›ç­”, æ˜¯ä¸æ˜¯ä¹Ÿå¯ä»¥æœ‰ä¸€ä¸ªè¾ƒé«˜çš„æ¦‚ç‡ç›¸ä¿¡ä»–æ˜¯æ­£ç¡®çš„**

é‡å¤æ‰“æ ‡, å¯ä»¥åšåˆ°
1. é‡å¤æ‰“æ ‡å¯ä»¥æ”¹å–„æ ‡ç­¾è´¨é‡å’Œæ¨¡å‹è´¨é‡, ä½†å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆçš„
2. å½“labelæ˜¯noisyçš„, é‡å¤æ‰“æ ‡æ¯”å•ç‹¬æ‰“æ ‡æ›´å¯å–, å³ä½¿æ˜¯åœ¨ä¼ ç»Ÿçš„labelingå¹¶ä¸cheapçš„æƒ…å½¢
3. åªè¦åŠ å·¥æ‰“æ ‡æ•°æ®çš„costä¸æ˜¯freeçš„, å³ä½¿ä¸ºæ‰€æœ‰éƒ½å¤šæ¬¡æ‰“æ ‡è¿™æ ·ç®€å•çš„ç­–ç•¥éƒ½å¯ä»¥å¾—åˆ°å¯è§‚çš„æ”¹å–„
4. ä¸ºä¸€ä¸ªå°å¿ƒé€‰æ‹©çš„dataé‡å¤æ‰“æ ‡é€šå¸¸æ¥è®²æ˜¯æ›´å¥½çš„, we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. 

This paper focuses on problems where it is possible to ob-
tain certain (noisy) data values (â€œlabelsâ€) relatively cheaply,
from multiple sources (â€œlabelersâ€). å…¶å®ä¹Ÿè¿˜æ˜¯å¤šæ•™å¸ˆ. æˆ‘ä»¬è¯¥æ€ä¹ˆå»åˆ©ç”¨è¿™ä¸€ç¥¨noisy, non-expertçš„labelå‘¢. 

é¢å¯¹noisyçš„æ ‡ç­¾æ—¶, éšç€é¢„å¤„ç†çš„costå’Œæ‰“æ ‡çš„costçš„æ¯”ç‡çš„æç¤º, é‡å¤æ‰“æ ‡è¿™ä¸ªæƒ³æ³•å°±å¾ˆè‡ªç„¶äº†: ä¸ºéƒ¨åˆ†æˆ–è€…æ‰€æœ‰çš„dataå¤šæ¬¡æ‰“æ ‡. æœ¬æ–‡å…³æ³¨whether, when, which(data)è¿™ä¸‰ç‚¹.
1. é€æ¸å‡å¼±å‡è®¾, è¯„ä¼°é‡å¤æ‰“æ ‡çš„å½±å“on the quality of the resultant labels, ç”±labelerså„è‡ªçš„confidenceå†³å®š.
2. å¿½ç•¥the cost of obtaining the unlabeled part of a data point, é‡å¤æ‰“æ ‡ä¹Ÿèƒ½å¤Ÿæœ‰å‡ å€çš„æ”¹è¿›

è‹¥æ¯ä¸ªlaberséƒ½æœ‰è‡ªå·±çš„qualities, é‚£ä¹ˆ, æ˜¯é€‰æ‹©æœ€å¥½çš„é‚£ä¸ª, è¿˜æ˜¯ç”¨å¤šä¸ª? ç›´è§‚åœ°æƒ³, ä¿¡æ¯å¤šåº”è¯¥ä¼šæœ‰ä¸ªå¥½ç»“æœ. äº‹å®ä¸Šä¹Ÿæ˜¯å¦‚æ­¤.

voting:
å¤§å¤šæ•°voteæ–¹æ³•éƒ½å¾ˆç›´æ¥, ä½†ä¼šå¯¼è‡´ä¸€ä¸ªç¼ºç‚¹: æ ‡ç­¾çš„ä¸ç¡®å®šæ€§ä¸¢å¤±.
æˆ‘ä»¬å¯ä»¥è¿™æ ·, assigns a weight 1/L, where L is the number of occurrences of this label in L.

# Efficiently learning the accuracy of labeling sources for selective sampling

>ThesecondtypeofALCmethods[Longetal., 2013][Don-
mez et al., 2009] [Zhao et al., 2014] does not assume all an-
notators will be have exactly the same. 

[Donmez et al., 2009] P. Donmez, J. G. Carbonell, and
J. Schneider. Efficiently learning the accuracy of labeling
sources for selective sampling. In KDD, pages 259â€“268.
ACM, 2009.

---

å†è®¸å¤šçš„æ•°æ®æŒ–æ˜åº”ç”¨ä¸­, ä¸ºtraining setå¾—åˆ°labelæ˜¯ä¸ªå¾ˆå®¹æ˜“å‡ºé”™çš„è¿‡ç¨‹, noisyçš„äº§ç”Ÿå¯èƒ½æœ‰å¾ˆå¤šåŸå› , æ¯”å¦‚ä¸»è§‚æƒ³æ³•çš„ä»‹å…¥, æ•°æ®ä¾èµ–, ä¸åˆé€‚çš„ç‰¹å¾ç­‰ç­‰. åœ¨å¤šæ•™å¸ˆæƒ…å½¢ä¸‹, ç”±äºannotatorçš„æ°´å¹³æ— æ³•æ§åˆ¶, class noiseæ˜¯ä¸€ä¸ªå¸¸è§çš„é—®é¢˜. æœ¬æ–‡, æˆ‘ä»¬ç›´é¢äº†è¿™æ ·çš„æŒ‘æˆ˜, ä»¥å¤šä¸ªä¸ç¡®å®šå…¶expertiseçš„noisy labelerä¸ºåŸºç¡€å»ä¸»åŠ¨æ‰“æ ‡. ç›®æ ‡æ˜¯ä¼°è®¡æ¯ä¸ªlaberçš„å‡†ç¡®æ€§ç„¶åä½¿ç”¨è¿™ä¸ªä¼°è®¡å»é€‰æ‹©æœ€å¥½çš„labelerså›ç­”å¯¹åº”çš„æé—®. ä½†æ˜¯, è¦æƒ³ç®—å‡ºæ¥ä¸€ä¸ªç¡®åˆ‡çš„reward functionæ˜¯ä¸å¯èƒ½çš„å› ä¸ºçœŸæ­£çš„æ ‡ç­¾æˆ‘ä»¬å¹¶ä¸çŸ¥é“(çœ‹æ¥è¿™æ˜¯æ²¡æœ‰é‡‘æ ‡æ•°æ®é›†ç›´æ¥ä»labelerså›ç­”ä¸­æ€»ç»“ç­”æ¡ˆ). æˆ‘ä»¬å‡è®¾è¿™äº›labelerè‡³å°‘æœ‰0.5çš„æ­£ç¡®ç‡, ç„¶åæˆ‘ä»¬é€‰æ‹©é‚£äº›å¤§å¤šæ•°çš„æ ‡ç­¾å½“ä½œç­”æ¡ˆ.

**idea, å®é™…ä¸Š, ä¼šä¸ä¼šæœ‰è¿™ç§å¯èƒ½, é¢å¯¹æŸä¸ªé—®é¢˜, çš„ç¡®æ˜¯ä¼šæœ‰å¤§å¤šæ•°äººéƒ½å›ç­”é”™è¿™ç§'éš¾é¢˜'å­˜åœ¨, èƒ½ä¸èƒ½æ£€æµ‹å‡ºæ¥è¿™ç§æƒ…å†µå†äº¤ç»™expertå»å›ç­”?**


# 11. An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data

>ThesecondtypeofALCmethods[Longetal., 2013][Don-
mez et al., 2009] [Zhao et al., 2014] does not assume all an-
notators will be have exactly the same. 

[Zhao et al., 2014] L. Zhao, Y. Zhang, and G. Sukthankar.
An active learning approach for jointly estimating worker
performance and annotation reliability with crowdsourced
data. arXiv preprint, 2014.

---

ä¸¤ç‚¹æŒ‘æˆ˜: æœ‰é™çš„annotatoräººå‘˜, label noise. å³ä½¿æ¯ä¸ªannotatoræ˜¯éå¸¸cheapçš„, ä½†æ˜¯æ€»ä½“costä¼šé†‰ç€è¯·æ±‚æ ‡ç­¾é‡çš„æç¤ºä¼šå¿«é€Ÿå¢é•¿.

æœ¬æ–‡ä¸­, ä½¿ç”¨äº†è¦ç»™å…³äº **Labels, Abilities, and Difficulties** çš„ç”Ÿæˆæ¨¡å‹, ä½¿ç”¨EMç®—æ³•æ¥å­¦ä¹ å‚æ•°.

![](http://otivusbsc.bkt.clouddn.com/3cf0ff46-669c-4913-b4fb-a33c46f52d50)


#  Efficiently learning the accuracy of labeling sources for selective sampling.

> In [Donmez et al., 2009] and [Long et al., 2013],
all instances are assumed to be of the same difficulty, and
different methods are proposed to estimate the reliability of
annotators.

[Donmez et al., 2009] P. Donmez, J. G. Carbonell, and
J. Schneider. Efficiently learning the accuracy of labeling
sources for selective sampling. In KDD, pages 259â€“268.
ACM, 2009.

---

ä¸»è¦åœ¨è¡¡é‡labelerçš„å¯é æ€§. 
å°±æ˜¯è¿™ç¯‡æ–‡ç« æå‡ºäº†æå‡ºäº†IEThresh (Interval Estimate  ) 

---

# Active Learning from Multiple Knowledge Sources
