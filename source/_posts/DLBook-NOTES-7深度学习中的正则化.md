---
title: DLBook NOTES 7深度学习中的正则化
date: 2017-07-26 09:50:40
tags: 
 - NOTES
 - DLBOOK
---

## 参数范数惩罚
$L_2$范数对于二次代价函数的影响: 只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函数减小的方向（对应 Hessian 矩阵较小的特征值）上改变参数不会显著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。

$L_2$正则化能让学习算法 ‘‘感知’’ 到具有较高方差的输入 $x$，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩.

相比$L_2$正则化，$L_1$正则化会产生更稀疏（sparse）的解。此处稀疏性指的是
最优值中的一些参数为 $0$。和$L_2$正则化相比，$L_1$正则化的稀疏性具有本质的不同. $L_2$正则化不会使参数变得稀疏，而$L_1$ 正则化有可能通过足够大的 α 实现稀疏。

由 $L_1$ 正则化导出的稀疏性质已经被广泛地用于特征选择（feature selection）机
制。

<!-- more -->

## 作为约束的范数惩罚
惩罚可能会导致目标函数非凸而使算法陷入局部极小 (对应于小的 θ）。

约束神经网络层的权重矩阵每列的范数，而不是限制整个权重矩阵的 Frobenius 范数。分别限制每一列的范数可以防止某一隐藏单元有非常大的权重。

## 噪声鲁棒性

另一种正则化模型的噪声使用方式是将其加到的权重。这项技术主要用于循环
神经网络 (Jim et al., 1996; Graves, 2011)。这可以被解释为关于权重的贝叶斯推断的
随机实现。贝叶斯学习过程将权重视为不确定的，并且可以通过概率分布表示这种
不确定性。向权重添加噪声是反映这种不确定性的一种实用的随机方法。
在某些假设下，施加于权重的噪声可以被解释为与更传统的正则化形式等同，
鼓励要学习的函数保持稳定。

## early stopping

在每次验证集误差有所改善后，我们存储模型参数的副本。当训练算法终止时，我们返回这些参数而不是最新的参数。当验证
集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止。
这可能是深度学习中最常用的正
则化形式。它的流行主要是因为有效性和简单性。

提前终止需要验证集，这意味着某些训练数据不能被馈送到模型。为了更好地
利用这一额外的数据，我们可以在完成提前终止的首次训练之后，进行额外的训练。
在第二轮额外的训练步骤中，所有的训练数据都被包括在内。有两个基本的策略都
可以用于第二轮训练过程:

1. 一个策略（算法7.2）是再次初始化模型，然后使用所有数据再次训练。在这个
第二轮训练过程中，我们使用第一轮提前终止训练确定的最佳步数。
2. 保持从第一轮训练获得的参数，然后使用全部的数据继续训练。
在这个阶段，已经没有验证集指导我们需要在训练多少步后终止。相反，我们可以监
控验证集的平均损失函数，并继续训练，直到它低于提前终止过程终止时的目标值。
此策略避免了重新训练模型的高成本，但表现并没有那么好。

## Bagging 和其他集成方法

分别训练几个不同的模型，然后让所有模型表决测
试样例的输出。这是机器学习中常规策略的一个例子，被称为模型平均（model
averaging）。采用这种策略的技术被称为集成方法。

模型平均（model averaging）奏效的原因是不同的模型通常不会在测试集上产
生完全相同的误差。





