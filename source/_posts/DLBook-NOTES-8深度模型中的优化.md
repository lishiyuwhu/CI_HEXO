---
title: DLBook-NOTES-8深度模型中的优化
date: 2017-09-14 10:05:59
tags: NOTES
---

## 8.1 学习和纯优化有什么不同

我们的目标是降低期望泛化误差, 也就是risk, 我们并不直接最优化风险, 而是去最优化经验风险. 但是, 经验风险的最小化很容易导致过拟合, 而且, 很多情况下这么做并不可行. 

代理损失函数
    代理损失函数作为原损失的代理，还具备一些优点。例如，正确类别的负对数似然通常用作 0−1 损失的替代。负对数似然允许模型估计给定样本的类别的条件概率，如果该模型效果好，那么它能够输出期望最小分类误差所对应的类别

提前终止
    和优化不同的是, 提前终止时代理损失函数仍然有较大的导数

batch, stochastic 和 online
    使用整个训练集的优化算法被称为批量（batch）或确定性（deterministic）梯度算法，因为它们会在一个大批量中同时处理所有样本。这个术语可能有点令人困惑，因为这个词 “批量’’ 也经常被用来描述小批量随机梯度下降算法中用到的小批量样本。通常，术语 “批量梯度下降’’ 指使用全部训练集，而术语 “批量’’ 单独出现时指一组样本。例如，我们普遍使用术语 “批量大小’’ 表示小批量的大小。
    现在, minibatch也被称作stochastic

可能是由于小批量在学习过程中加入了噪声，它们会有一些 **正则化效果**  (Wilson and Martinez, 2003)。泛化误差通常在批量大小为 1 时最好。因为梯度估计的高方差，小批量训练需要 **较小的学习率** 以保持稳定性。因为降低的学习率和消耗更多步骤来遍历整个训练集都会产生更多的步骤，所以会导致总的运行时间非常大。

有因为minibatch引入了噪声, 所以不同的更新方式要有恰当的batch. 仅基于梯度 g 的更新方法通常相对鲁棒，并能使用较小的批量获得成功，如 100。使用Hessian矩阵 H，计算如 H −1 g 更新的二阶方法通常需要更大的批量，如 10,000


<!-- more -->

## 8.2 神经网络优化中的挑战

病态问题一般被认为存在于神经网络训练过程中。病态体现在随机梯度下降会‘‘卡’’ 在某些情况，此时即使很小的更新步长也会增加代价函数。

实际上, 对于NN, 大部分地方可能是很平坦的, 代价很高的局部极小值会很稀有. 

更加令人关注的是鞍点, 而且很不幸的是, 真实的NN中也存在包含很多高代价鞍点的损失函数.

悬崖和梯度爆炸(RNN中经常看到)
    启发式梯度截断(gradient clipping) : 其基本想法源自梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。当传统的梯度下降算法提议更新很大一步时，启发式梯度截断会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。


## 8.3 基本算法

### SGD

保证SGD收敛的一个充分条件是: 学习率的sum等于无穷大, 平方和小于无穷大. 实作中, 线性的衰减学习率, 在一定步数之后使学习率保持常数.

关于学习率:

![](http://otivusbsc.bkt.clouddn.com/0d67e28a-fc0a-4138-8860-bd15ef4eef1f)

### 动量 momentum

![](http://otivusbsc.bkt.clouddn.com/0423f958-d1be-4dd6-9403-00d710ae7a73)

实际上, 动量法的超参是$\frac{1}{1-\alpha}$


## 8.4 参数初始化策略


