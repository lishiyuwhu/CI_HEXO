---
title: 从LeNet到DenseNet: CNN的进化之路
date: 2017/11/30  12:58
tags: ML
toc: true
---

这篇是大致回顾一下一路走来的这些模型, 以后一些和具体模型联系比较紧密的东西也可以有地方放了.

## LeNet-5

LeNet-5是Yann LeCun在1998年设计的用于手写数字识别的卷积神经网络，是早期卷积神经网络中最有代表性的实验系统之一。

LeNet-5的这张图应该算是随处可见了.

![](http://otivusbsc.bkt.clouddn.com/77400155-7ecb-478c-ba50-07b92e2cfc71)

除了S2和C3的连接比较奇怪其他都是很容易理解的.

可以注意下的几点:

filter的大小是固定5*5, pooling层是2*2的maxpooling, 没有overlap, sigmoid的激活函数


<!-- more -->

## AlexNet

2012 年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 创造了一个“大型的深度卷积神经网络”，也即现在众所周知的 AlexNet，赢得了当年的 ILSVRC冠军。

AlexNet:
 - 训练出当时最大规模的卷积神经网络
 - 实现高效的GPU卷积运算结构，也使得此后GPU成为深度学习的主要工具
 - 通过众多的skills（dropout、RELU、Data Augmentation）解决深层神经网络的过拟合问题，使得该网络在60 million参数的情况下依旧能很好收敛。这些方法现在已经成为了CNN不可或缺的一部分。

![](http://otivusbsc.bkt.clouddn.com/cc3429a9-0fea-4879-ac60-fd497319ed7a)

###  GPU

单个GTX 580 GPU只有3GB内存，这限制了可以在其上训练的网络的最大规模. 120万个训练样本才足以训练网络，这网络太大了，不适合在一个GPU上训练. --> 将网络分布在两个GPU上, 在每个GPU中放置一半核（或神经元）.

### Relu

传统的sigmoid和tanh是*饱和*的, 就是输入值达到一定大小之后就变化很小了. 而Relu则是*非饱和*的.

Sigmoid函数在正负饱和区的梯度都接近于0，所以会造成梯度弥散. 

relu函数在负半区的导数为0，一旦神经元激活值进入负半区，那么梯度就会为0，这个神经元不会经历训练，即所谓的稀疏性，起到了类似L1的正则化作用，可以在一定程度上缓解过拟合.

同时, Relu的导数非零即1易于计算(忽略0点的不可导性)

>当然，ReLU也是有缺点的，比如左边全部关了很容易导致某些隐藏节点永无翻身之日，所以后来又出现pReLU、random ReLU等改进，而且ReLU会很容易改变数据的分布，因此ReLU后加Batch Normalization也是常用的改进的方法。

### LRN层

局部响应归一化层（Local Response Normalization Layer）. 

基本被认为是没大用的东西. 大牛提出, 然后caffe历史遗留产物.

### Data augmentation

平移, 镜像, corp等等, 现在基本上用的也比较多了. 和改变RGB通道强度这种一起算作data预处理过程.

### Dropout

通常来说, Dropout都被认作是类似于bagging的一种方法. 


## VGG

ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。

代表性的VGG-19
![](http://otivusbsc.bkt.clouddn.com/fe5442f0-c8cc-49bf-9f48-2e40ccef15f0)

VGG是在从Alex-net发展而来的网络。最关键的一点是使用小卷积核. 

>引入cs231n上面一段话：几个小滤波器卷积层的组合比一个大滤波器卷积层好：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有C个通道，那么单独的7x7卷积层将会包含7*7*C=49C2个参数，而3个3x3的卷积层的组合仅有个3*（3*3*C）=27C2个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。1*1 filter: 作用是在不影响输入输出维数的情况下，对输入线进行线性形变，然后通过Relu进行非线性处理，增加网络的非线性表达能力。


虽然VGG比Alex-net有更多的参数，更深的层次, 但是VGG只需要很少的迭代次数就开始收敛: 
 
 - 更深的深度和小的卷积核起到了隐式正则化作用。
 这点其实挺令人迷惑的, 人人都有自己的说法, 我来说说目前我自己的看法. 更深的深度, 一定程度下是可以提高整个网络的能力, 但没有经过特殊设计的网络结构加深反而会出问题, 前半句个人还是存疑的. 小的卷积核, 小的stride, 直观上来讲就是将一个大卷积核分散在了多个layer当中, 那相应的也经历了pooling和activate function, 就像是一个NN构成了这个大的卷积核, 里面小的核在投票.  

- 一些层的pre-initialisationpre-initialisation：网络A的权值W~（0,0.01）的高斯分布，bias为0；由于存在大量的ReLU函数，不好的权值初始值对于网络训练影响较大。为了绕开这个问题，作者现在通过随机的方式训练最浅的网络A；然后在训练其他网络时，把A的前4个卷基层（感觉是每个阶段的以第一卷积层）和最后全连接层的权值当做其他网络的初始值，未赋值的中间层通过随机初始化。


### GoogLeNet一家子---Inception v1

VGG给人带来了启发, 更深的网络可能就能够带来更好的表达能力. 但并不是说深就能深的, 网络的设计是至关重要的一环. 

 GoogLeNet Inception v1是GoogLeNet的最早版本，出现在2014年的《Going deeper with convolutions》。之所以名为“GoogLeNet”而非“GoogleNet”,文章说是为了向早期的LeNet致敬。而Inception这个名字更是令人浮想联翩, 第一瞬间就
 
![](http://otivusbsc.bkt.clouddn.com/62c63497-b64d-4d70-8660-6aa775c40382)

鬼谲的记忆植入, 一层一层的梦境就像是NN中一层一层的反馈. 

key: 稀疏连接:

 - 一方面现实生物神经系统的连接也是稀疏的，
 
 - 另一方面有文献表明：对于大规模稀疏的神经网络，可以通过分析激活值的统计特性和对高度相关的输出进行聚类来逐层构建出一个最优网络。这点表明臃肿的稀疏网络可能被不失性能地简化。

所以，现在的问题是有没有一种方法，既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。大量的文献表明可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，据此论文提出了名为Inception 的结构来实现此目的。

![](http://otivusbsc.bkt.clouddn.com/064d0e96-0558-4dbd-8c52-5c1aa3d12fa6)

1 . 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； 

2 . 之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了； 

3 . 文章说很多地方都表明pooling挺有效，所以Inception里面也嵌入了。 

4 . 网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。

实际上卷积的使用就会带来稀疏性. 这里引人注目的是这个3*3 的pooling, 看看resnet, 是不是有几分神似. 这样的一个pooling, 实际上可以看作上层的直接信息输入, 目前来看, 直接引入上层的信息会有相当好的效果. 一定程度上也减少了train过程中梯度消失和爆炸. 和后面具体结构图中看到的前两个softmax一个效果.

但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴'Network in network'，采用1x1卷积核来进行降维。 

例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256，大约减少了4倍。

改进为
![](http://otivusbsc.bkt.clouddn.com/f5007fdf-b9ce-4853-af93-2ffff67e2eda)

最后的GoogLeNet（Inception-V1）的结构图如下：

![](http://otivusbsc.bkt.clouddn.com/77c10c44-4922-42e0-94f2-5713527ba23f)

1 . 显然GoogLeNet采用了模块化的结构，方便增添和修改； 
2 . 网络最后采用了average pooling来代替全连接层，想法来自NIN,事实证明可以将TOP1 accuracy提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便以后大家finetune； 
3 . 虽然移除了全连接，但是网络中依然使用了Dropout ; 
4 . 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。文章中说这两个辅助的分类器的loss应该加一个衰减系数，但看caffe中的model也没有加任何衰减。此外，实际测试的时候，这两个额外的softmax会被去掉。

### GoogLeNet一家子---Inception v2v3

《Rethinking the Inception Architecture for Computer Vision》, V1的进化版. 

首先*通过实验*给出了一些已经被证明有效的用于放大网络的通用准则和优化方法。这些准则和方法适用但不局限于Inception结构。


1 . 避免表达瓶颈，特别是在网络靠前的地方。 信息流前向传播过程中显然不能经过高度压缩的层，即表达瓶颈。从input到output，feature map的宽和高基本都会逐渐变小，但是不能一下子就变得很小。比如你上来就来个kernel = 7, stride = 5 ,这样显然不合适。 另外输出的维度channel，一般来说会逐渐增多(每层的num_output)，否则网络会很难训练。（特征维度并不代表信息的多少，只是作为一种估计的手段）

2 . 高维特征更易处理。 高维特征更易区分，会加快训练。

3. 可以在低维嵌入上进行空间汇聚而无需担心丢失很多信息。 比如在进行3x3卷积之前，可以对输入先进行降维而不会产生严重的后果。假设信息可以被简单压缩，那么训练就会加快。

4 . 平衡网络的宽度与深度。



再来说V2在哪里改进了.

首先就死, *用两个连续的3*3卷积层来代替5*5的kernel*. 带来的两个问题: 

1 . 这种替代会造成表达能力的下降吗？ 
后面有大量实验可以表明不会造成表达缺失；

2 . 3x3卷积之后还要再加激活吗？ 
作者也做了对比试验，表明添加非线性激活会提高性能。

从这里出发, 3*3还能怎么变得更小呢? 采用1*n卷积核

其次，*引入非对称卷积*。例如，将3x1的卷积和1x3的卷积串联起来，与直接进行3x3卷积的结果是等价的。这种卷积方式大大降低了参数量，从nxn降到了2xn，所以当n越大，降低得越多。实际上，作者发现在网络的前期使用这种分解效果并不好，还有在中度大小的feature map上使用效果才会更好。（对于mxm大小的feature map,建议m在12到20之间）。

![](http://otivusbsc.bkt.clouddn.com/621e0ea9-9ca8-4ab9-b6c4-be2f574f0dbb)

1. 图4是GoogLeNet V1中使用的Inception结构；
2. 图5是用3x3卷积序列来代替大卷积核；
3. 图6是用nx1卷积来代替大卷积核，这里设定n=7来应对17x17大小的feature map。该结构被正式用在GoogLeNet V2中。即非对称个卷积核，其实类似于卷积运算中，二维分解为1维计算，提高了计算速度。

第三种模式会在比较深的地方才采用.


BN方面会放在这里
https://lishiyuwhu.github.io/2017/11/22/Batch%20normalization/


LSR:


## ResNet



![](http://otivusbsc.bkt.clouddn.com/75360de3-9e42-4dee-afa4-6f1b3241bae1)



-------



参考:

http://blog.csdn.net/KangRoger/article/details/69218625

https://zhuanlan.zhihu.com/p/31006686

http://blog.csdn.net/shuzfan/article/details/50738394

http://kaiminghe.com/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf

