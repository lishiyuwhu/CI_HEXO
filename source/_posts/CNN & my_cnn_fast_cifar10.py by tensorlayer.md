---
title: CNN & my_cnn_fast_cifar10.py by tensorlayer
date: 2017-07-30 17:04:10
tags:
---

## Convolutional Neural Network
CNNæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èŒƒä¾‹, å®ƒå‘Šè¯‰äº†æˆ‘ä»¬, NNçš„æ¶æ„å¯ä»¥ç”±æˆ‘ä»¬è‡ªå·±è®¾è®¡å¹¶ä¸”è¡¨ç°å¾—å¾ˆå¥½

åœ¨ä½ æƒ³ç”¨ä¸€ä¸ªfeatureå»æ¯”å¯¹å›¾ç‰‡çš„æ—¶å€™, æœ‰ä¸‰ä¸ªé‡ç‚¹:

 - pattern
 - location
 - subsampling

## åŸºæœ¬æ¶æ„

image -> convolution -> pooling-> Flatten -> Fully Connected Feedforward Network -> softmax ->output

å…¶ä¸­, å·ç§¯å±‚è§£å†³äº†patternå’Œlocationé—®é¢˜, æ± åŒ–å±‚è§£å†³äº†subsaplingé—®é¢˜

<!-- more -->

### conv
å‡è®¾ä¸€ä¸ª6\*6\*1çš„imageæ‰”åˆ°è¿™ä¸ªCNNä¸­, ç¬¬ä¸€ä¸ªconv, å–ä¸€ä¸ª3\*3çš„filter(kernel or neuron)ä¸imageä¸­ç›¸åŒå¤§å°å¿«ä¾æ¬¡åšå†…ç§¯, æ¯æ¬¡ç§»åŠ¨1ä¸ªpix(stride=1), äºæ˜¯å¾—åˆ°äº†ä¸€ä¸ª4\*4çš„image(ä¸åšpadding), å‡è®¾æˆ‘ä»¬æœ‰nä¸ªfilter, é‚£ä¹ˆé€šè¿‡è¿™ä¸ªconvå°±å¾—åˆ°äº†ä¸€ä¸ª 4\*4\*nçš„Feature Map

å¦‚æœæ˜¯ä¸ªRGBå›¾åƒ, å³6\*6\*3, é‚£ä¹ˆæˆ‘ä»¬çš„filterä¹Ÿè¦æ˜¯3\*3\*3çš„, æ¯ä¸ªsliceå¯¹åº”imageçš„ä¸€ä¸ªchannel.

äº‹å®ä¸Š,  æˆ‘ä»¬ä»”ç»†åˆ†ææ¯ä¸ªfilteråšäº†ä»€ä¹ˆçš„è¯, å¦‚æœæŠŠä¸€ä¸ªfiltçœ‹ä½œä¸€ä¸ªneuron, filterå†…ç§¯çš„ç‰¹æ€§, åšåˆ°äº†ä¸¤ä»¶äº‹:

 - parameter sharing
 - drop connect
 
![ ](http://otivusbsc.bkt.clouddn.com/752e7d7d-6469-42b3-9e58-5d5ae31d005b)


é¡ºå¸¦ä¸€è¯´'å·ç§¯'è¿™ä¸ªæ¦‚å¿µ. å¾ˆå¤šäººéƒ½è¯´è¿™ä¸ªæ€ä¹ˆå°±æ˜¯å·ç§¯äº†, ä¸å°±æ˜¯ç®€å•çš„ä¸€ä¸ªinner productionå—?
æ€ä¹ˆçœ‹ä¹Ÿä¸æ˜¯é‚£ä¸ªç§¯åˆ†u(t)h(a-t)å½¢å¼. å…¶å®, è¦æ˜¯ä»ç¦»æ•£çš„è§’åº¦å»çœ‹å·ç§¯è¿ç®—, 

>ä¸å…¶ç†è§£æˆç¿»è½¬ï¼Œä¸å¦‚ç†è§£æˆå»¶è¿Ÿåå åŠ ã€‚

è¿™å¥è¯æ˜¯æˆ‘è§‰å¾—è¯´çš„éå¸¸å¥½çš„ä¸€ä¸ªæ­ç¤ºå·ç§¯æœ¬è´¨çš„è¯. å†å»çœ‹çœ‹filterçš„è¡Œä¸ºå°±å¾ˆæ˜äº†äº†.


### padding

æ ¹æ®tensorflowä¸­çš„conv2då‡½æ•°ï¼Œæˆ‘ä»¬å…ˆå®šä¹‰å‡ ä¸ªåŸºæœ¬ç¬¦å·

1ã€è¾“å…¥çŸ©é˜µ WÃ—Wï¼Œè¿™é‡Œåªè€ƒè™‘è¾“å…¥å®½é«˜ç›¸ç­‰çš„æƒ…å†µï¼Œå¦‚æœä¸ç›¸ç­‰ï¼Œæ¨å¯¼æ–¹æ³•ä¸€æ ·ï¼Œä¸å¤šè§£é‡Šã€‚

2ã€filterçŸ©é˜µ FÃ—Fï¼Œå·ç§¯æ ¸

3ã€strideå€¼ Sï¼Œæ­¥é•¿

4ã€è¾“å‡ºå®½é«˜ä¸º new_heightã€new_width

å½“ç„¶è¿˜æœ‰å…¶ä»–çš„ä¸€äº›å…·ä½“çš„å‚æ•°ï¼Œè¿™é‡Œå°±ä¸å†è¯´æ˜äº†ã€‚

æˆ‘ä»¬çŸ¥é“ï¼Œpaddingçš„æ–¹å¼åœ¨tensorflowé‡Œåˆ†ä¸¤ç§ï¼Œä¸€ç§æ˜¯VALIDï¼Œä¸€ç§æ˜¯SAMEï¼Œä¸‹é¢åˆ†åˆ«ä»‹ç»è¿™ä¸¤ç§æ–¹å¼çš„å®é™…æ“ä½œæ–¹æ³•ã€‚

1ã€å¦‚æœpadding = â€˜VALIDâ€™

new_height = new_width = (W â€“ F + 1) / S ï¼ˆç»“æœå‘ä¸Šå–æ•´ï¼‰
ä¹Ÿå°±æ˜¯è¯´ï¼Œconv2dçš„VALIDæ–¹å¼ä¸ä¼šåœ¨åŸæœ‰è¾“å…¥çš„åŸºç¡€ä¸Šæ·»åŠ æ–°çš„åƒç´ ï¼ˆå‡å®šæˆ‘ä»¬çš„è¾“å…¥æ˜¯å›¾ç‰‡æ•°æ®ï¼Œå› ä¸ºåªæœ‰å›¾ç‰‡æ‰æœ‰åƒç´ ï¼‰ï¼Œè¾“å‡ºçŸ©é˜µçš„å¤§å°ç›´æ¥æŒ‰ç…§å…¬å¼è®¡ç®—å³å¯ã€‚

2ã€å¦‚æœpadding = â€˜SAMEâ€™

new_height = new_width = W / S ï¼ˆç»“æœå‘ä¸Šå–æ•´ï¼‰

paddingè¿™æ®µæ¥è‡ªäº

>ä½œè€…ï¼šTraphix
é“¾æ¥ï¼šhttp://www.jianshu.com/p/05c4f1621c7e
ä¾†æºï¼šç®€ä¹¦
è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

### pooling
å¾ˆå¤šæ—¶å€™æˆ‘ä»¬éƒ½æ˜¯maxpooling, å…¶å®averagepoolingä¸€æ ·ç”¨çš„å¾ˆå¤š, ä¾‹å¦‚åœ¨cifar-10çš„cnnè®¾è®¡ä¸­, avgpoolingè¢«é‡‡ç”¨äº†åœ¨åä¸¤cnovå±‚å
![ ](http://otivusbsc.bkt.clouddn.com/5a95606c-3f20-49c5-978f-cc23661c3764)

å–kernel=(2,2)çš„maxpoolingçš„è¯
6\*6 -> conv -> maxpooling -> 2\*2\*n

### flatten

ç›´æ¥æŠŠç»è¿‡poolingå¾—åˆ°çš„2\*2\*næ‹‰ç›´ä¸ºä¸€ä¸ªå‘é‡, æ¯ä¸ªsliceå¤´å°¾ç›¸æ¥, å‡è®¾n=16, è¿™æ—¶æˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ª64dimçš„vector, å†æ‰”è¿›ä¸€ä¸ªMLP, è¾“å‡ºä¸€ä¸ª10dim(å‡è®¾ä½œåœ¨mnistä¸Š), æœ€åæ¥ä¸Šä¸€ä¸ªsoftmax, å°±å¾—å‡ºåˆ†ç±»ç»“æœ


# my_cnn_fast_cifar10.py
ä¸‹é¢æ˜¯æˆ‘æŒ‰ç…§poolingé‚£é‡Œçš„cifar-10 fast modelå†™çš„ä»£ç , backendæ˜¯tensorflow, æ¡†æ¶tensorlayer.
```python
#! /usr/bin/python
# -*- coding: utf8 -*-

import tensorflow as tf
import tensorlayer as tl
import tensorlayer.layers as tll
import numpy as np
import time, os, io

sess = tf.InteractiveSession()

#train param
n_epoch = 500
learning_rate = 0.0001
print_freq = 1
batch_size = 128

#load cifar10
X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(
                                    shape=(-1, 32, 32, 3), plotable=False)
#InitW(Gaussian with zero mean)
W_init1 = tf.truncated_normal_initializer(stddev=0.0001)
W_init2 = tf.truncated_normal_initializer(stddev=0.001)
W_init3 = W_init2
W_init4 = tf.truncated_normal_initializer(stddev=0.1)
W_init5 = W_init4
b_init4 = tf.truncated_normal_initializer(stddev=0.1)
b_init5 = b_init4

#placeholder
x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name='x')
y_ = tf.placeholder(tf.int64, shape=[None, ], name='y_')

#model
nn = tll.InputLayer(x, name='Input1')
nn = tll.Conv2dLayer(nn, act=tf.nn.relu, shape=[5, 5, 3, 32], 
        strides=[1, 1, 1, 1], padding='SAME', W_init=W_init1, name='cnn1')
#output : (batchsize, 32, 32, 32)
nn = tll.PoolLayer(nn, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], 
        padding='SAME', pool= tf.nn.max_pool, name='pool1_max')
#output : (batchsize, 16, 16, 32)
nn = tll.Conv2dLayer(nn, act=tf.nn.relu, shape=[5, 5, 32, 32], 
        strides=[1, 1, 1, 1], padding='SAME', W_init=W_init2, name='cnn2')
#output : (batchsize, 16, 16, 32)
nn = tll.PoolLayer(nn, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], 
        padding='SAME', pool= tf.nn.avg_pool, name='pool2_avg')
#output : (batchsize, 8, 8, 32)
nn = tll.Conv2dLayer(nn, act=tf.nn.relu, shape=[5, 5, 32, 64], 
        strides=[1, 1, 1, 1], padding='SAME', W_init=W_init3, name='cnn3')
#output : (batchsize, 8, 8, 64)
nn = tll.PoolLayer(nn, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], 
        padding='SAME', pool= tf.nn.avg_pool, name='pool3_avg')
#output : (batchsize, 4, 4, 64)
nn = tll.FlattenLayer(nn, name='Flattenlayer')
nn = tll.DenseLayer(nn, n_units=64, act=tf.nn.relu, 
        W_init=W_init4, b_init=b_init4, name='relu1')
nn = tll.DenseLayer(nn, n_units=10, act=tf.identity, 
        W_init=W_init5, b_init=b_init5, name='output1')
y = nn.outputs

#cost
ce = tl.cost.cross_entropy(y, y_, name='cost_cross_entropy')
L2 = 0
for p in tl.layers.get_variables_with_name('relu/W', train_only=True, printable=True):
    L2 += tf.contrib.layers.l2_regularizer(0.004)(p)
cost = ce + L2

#acc
correct_prediction = tf.equal(tf.argmax(y, 1), y_)
acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

#train_op
train_params = nn.all_params
train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
    epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)

tl.layers.initialize_global_variables(sess)

print('****')
nn.print_params(False)
print('****')
nn.print_layers()
print('****')

print('    learning_rate: %f' % learning_rate)
print('    batch_size: %d' % batch_size)
```
æ­å¥½äº†æ¨¡å‹, ç„¶åå‘ç°å¿˜äº†å†™BNå±‚äº†, ğŸ¤£æ˜ä¸ªå†è¯´
è¯´ä¸ªç‚¹
![ ](http://otivusbsc.bkt.clouddn.com/5a95606c-3f20-49c5-978f-cc23661c3764)
è¿™é‡Œé¢, å·ç§¯å±‚å‚æ•°, ç»™äº†kernelä¹Ÿå°±æ˜¯filterçš„å¤§å°5/*5, ä½†æ²¡ç»™strides, å´ç»™çš„padding=2. è§è¿‡padding='SAME' or 'VAILD', è¿™ä¸ªæ•°å­—æ˜¯ä¸ªå•¥. 

ç ”ç©¶äº†ä¸€ä¸‹, åº”è¯¥å°±æ˜¯é»˜è®¤padding='SAME' , ç„¶åçœ‹è¿™ä¸ªå›¾
![](http://otivusbsc.bkt.clouddn.com/a56f0c24-ab26-406d-803f-1fbcea7377f9)

padding=2 å°±æ˜¯æœ€å¤–å±‚å¥—2ä¸ªpixel, é‚£ä¹ˆstrides=1å°±å¾ˆæ˜¾ç„¶, ä¸ç„¶å–2å¹²ä»€ä¹ˆ.

---

èµ°ä¹‹å‰æ”¹äº†ä¸‹åˆæ­¥, åŠ äº†ä¸ªBathchnormallayer

```python
#! /usr/bin/python
# -*- coding: utf8 -*-

import tensorflow as tf
import tensorlayer as tl
import tensorlayer.layers as tll
import numpy as np
import time, os, io

sess = tf.InteractiveSession()

#load cifar10
X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(
                                    shape=(-1, 32, 32, 3), plotable=False)

def fast_model(x, y_, reuse, is_train):
    #InitW(Gaussian with zero mean)
    W_init1 = tf.truncated_normal_initializer(stddev=0.0001)
    W_init2 = tf.truncated_normal_initializer(stddev=0.001)
    W_init3 = W_init2
    W_init4 = tf.truncated_normal_initializer(stddev=0.1)
    W_init5 = W_init4
    b_init4 = tf.truncated_normal_initializer(stddev=0.1)
    b_init5 = b_init4
    
    with tf.variable_scope("model", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        #model
        nn = tll.InputLayer(x, name='Input1')
        nn = tll.Conv2dLayer(nn, act=tf.nn.relu, shape=[5, 5, 3, 32], 
                strides=[1, 1, 1, 1], padding='SAME', W_init=W_init1, name='cnn1')
        #output : (batchsize, 32, 32, 32)
        nn = tll.BatchNormLayer(nn, is_train, act=tf.nn.relu, name='batch1')
        nn = tll.PoolLayer(nn, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], 
                padding='SAME', pool= tf.nn.max_pool, name='pool1_max')
        #output : (batchsize, 16, 16, 32)
        nn = tll.Conv2dLayer(nn, act=tf.nn.relu, shape=[5, 5, 32, 32], 
                strides=[1, 1, 1, 1], padding='SAME', W_init=W_init2, name='cnn2')
        #output : (batchsize, 16, 16, 32)
        nn = tll.BatchNormLayer(nn, is_train, act=tf.nn.relu, name='batch2')
        nn = tll.PoolLayer(nn, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], 
                padding='SAME', pool= tf.nn.avg_pool, name='pool2_avg')
        #output : (batchsize, 8, 8, 32)
        nn = tll.Conv2dLayer(nn, act=tf.nn.relu, shape=[5, 5, 32, 64], 
                strides=[1, 1, 1, 1], padding='SAME', W_init=W_init3, name='cnn3')
        #output : (batchsize, 8, 8, 64)
        nn = tll.BatchNormLayer(nn, is_train, act=tf.nn.relu, name='batch3')
        nn = tll.PoolLayer(nn, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], 
                padding='SAME', pool= tf.nn.avg_pool, name='pool3_avg')
        #output : (batchsize, 4, 4, 64)
        nn = tll.FlattenLayer(nn, name='Flattenlayer')
        nn = tll.DenseLayer(nn, n_units=64, act=tf.nn.relu, 
                W_init=W_init4, b_init=b_init4, name='relu1')
        nn = tll.DenseLayer(nn, n_units=10, act=tf.identity, 
                W_init=W_init5, b_init=b_init5, name='output1')
        y = nn.outputs

        #cost
        ce = tl.cost.cross_entropy(y, y_, name='cost_cross_entropy')
        L2 = 0
        for p in tl.layers.get_variables_with_name('relu/W', train_only=True, printable=True):
           L2 += tf.contrib.layers.l2_regularizer(0.004)(p)
        cost = ce + L2

        #acc
        correct_prediction = tf.equal(tf.argmax(y, 1), y_)
        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        
        return nn, cost, acc


#train param
n_epoch = 500
learning_rate = 0.0001
print_freq = 1
batch_size = 128

#placeholder
x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name='x')
y_ = tf.placeholder(tf.int64, shape=[None, ], name='y_')

nn, cost, _ = fast_model(x, y_, False, is_train=True)
_, cost_test, acc = fast_model(x, y_, True, is_train=False)


#train_op
train_params = nn.all_params
train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
    epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)

tl.layers.initialize_global_variables(sess)

print('****')
nn.print_params(False)
print('****')
nn.print_layers()
print('****')

print('    learning_rate: %f' % learning_rate)
print('    batch_size: %d' % batch_size)

'''
tl.utils.fit(sess, nn, train_op, cost, X_train, y_train, x, y_,
           acc=acc, batch_size=batch_size, n_epoch=n_epoch, print_freq=5,
           X_val=None, y_val=None, eval_train=False,
           tensorboard=True)

tl.utils.test(sess, nn, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)

sess.close()
'''
```
num of params: 146090, 
å‚æ•°æ•°é‡æ¯”æœ€åŸå§‹ç‰ˆå°‘äº†ä¸€ä¸ªæ•°é‡çº§. å¾ˆè¿·, æ˜¯ä¸æ˜¯å†™é”™äº†, æ€ä¹ˆå°‘äº†è¿™ä¹ˆå¤š. æ‰¾ä¸ªæ—¶é—´trainä¸‹è¯•è¯•å†è¯´


