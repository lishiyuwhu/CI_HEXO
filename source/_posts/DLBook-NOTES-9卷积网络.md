---
title: DLBook-NOTES-9卷积网络
date: 2017-10-11 14:03:36
tags: NOTES
---

处理对象: 图像数据, **时间序列数据**

## 卷积运算

![图片描述](http://otivusbsc.bkt.clouddn.com//42f2b5a8-4043-4c94-a44c-018308c70765)
这个情境中, 卷积就起到了一个降噪的的作用, 也是为什么说, 卷积会让函数变得平滑.

**将小的局部区域上的相同线性变换应用到整个输入上，卷积是描述这种变换的极其有效的方
法。**

实际上, ML里$\int x(a)\omega(t+a)$也是叫做卷积, 被称作**互相关函数**. 原本定义的卷积对核函数进行了flip使得有了可交换性.

## 动机

稀疏交互（sparse interactions）、参数共享（parameter sharing）、等变表示（equivariant representations. 很明确的三个目标, 实现的方法也很自然. 对应于图像的性质.

sparse weight: 
![图片描述](http://otivusbsc.bkt.clouddn.com//8fb34dd8-e288-42e3-be64-1dca87da3ea0)

parameter sharing:  这虽然没有改变前向传播的运行时间（仍然是 O(k × n)），但它显著地把模型的存储需求
降低至 k 个参数，并且 k 通常要比 m 小很多个数量级。因为 m 和 n 通常有着大致相同的大小，k 在实际中相对于 m × n 是很小的。

## 池化

一步: convolution-> activation -> pooling

**使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。**

同时, 池化层可以用来调整不同大小的输入.


## 卷积与池化作为一种无限强的先验

一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。

与任何其他先验类似，卷积和池化只有当先验的假设合理且正确时才有用。如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差

## 基本卷积函数的变体

